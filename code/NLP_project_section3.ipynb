{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR0WzZVnpRNI"
      },
      "source": [
        "# Week 38 \n",
        "In this section, we implement three binary classifiers to predict wheher a question is answerable by the data set or not. \n",
        "\n",
        "The first model is a Logistic Regression (GloVe), where we use the gensim library by translating each data set to english.\n",
        "\n",
        "The second is a Logistic Regression with the BPEmb tokenizer; that is, we take the words from the langaueg and put them into smaller parts, and learn embeddings from these parts. We \n",
        "\n",
        "The third is a fine-tuning of the pre-trained bert-base-multilingual-cased model.\n",
        "\n",
        "Results and further explanations can be found in the report; overall, BERT outperforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjjvXNaWGSWA",
        "outputId": "72fd83a0-1292-4601-c1c0-de3b6502f99a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /Library/anaconda3/lib/python3.9/site-packages (2.14.5)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: aiohttp in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (1.20.3)\n",
            "Requirement already satisfied: xxhash in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: packaging in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (2.26.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: multiprocess in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (8.0.0)\n",
            "Requirement already satisfied: pandas in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (1.3.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /Library/anaconda3/lib/python3.9/site-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /Library/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /Library/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /Library/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
            "Requirement already satisfied: six>=1.5 in /Library/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: googletrans==3.1.0a0 in /Library/anaconda3/lib/python3.9/site-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /Library/anaconda3/lib/python3.9/site-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: idna==2.* in /Library/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: chardet==3.* in /Library/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: certifi in /Library/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.10.8)\n",
            "Requirement already satisfied: sniffio in /Library/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /Library/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /Library/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: hstspreload in /Library/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2023.1.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /Library/anaconda3/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /Library/anaconda3/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /Library/anaconda3/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /Library/anaconda3/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: spicy in /Library/anaconda3/lib/python3.9/site-packages (0.16.0)\n",
            "Requirement already satisfied: scipy in /Library/anaconda3/lib/python3.9/site-packages (from spicy) (1.7.1)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /Library/anaconda3/lib/python3.9/site-packages (from scipy->spicy) (1.20.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: accelerate in /Library/anaconda3/lib/python3.9/site-packages (0.24.1)\n",
            "Requirement already satisfied: pyyaml in /Library/anaconda3/lib/python3.9/site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Library/anaconda3/lib/python3.9/site-packages (from accelerate) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/anaconda3/lib/python3.9/site-packages (from accelerate) (1.20.3)\n",
            "Requirement already satisfied: huggingface-hub in /Library/anaconda3/lib/python3.9/site-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: torch>=1.10.0 in /Library/anaconda3/lib/python3.9/site-packages (from accelerate) (1.12.1)\n",
            "Requirement already satisfied: psutil in /Library/anaconda3/lib/python3.9/site-packages (from accelerate) (5.8.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /Library/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->accelerate) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /Library/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
            "Requirement already satisfied: requests in /Library/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.26.0)\n",
            "Requirement already satisfied: filelock in /Library/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (3.3.1)\n",
            "Requirement already satisfied: fsspec in /Library/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Library/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.62.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2021.10.8)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "zsh:1: no matches found: transformers[torch]\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets\n",
        "%pip install googletrans==3.1.0a0\n",
        "%pip install spicy\n",
        "%pip install accelerate -U\n",
        "%pip install transformers[torch]\n",
        "%pip install bpemb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO2h0PZWxCR0"
      },
      "outputs": [],
      "source": [
        "#%run dict_maker_lang.py generate dictionaries (they are in the .pkl files, this one takes 3 hours to run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yR-SFZwMv4M-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\kaspe\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kaspe\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "import pickle\n",
        "from bpemb import BPEmb\n",
        "# stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    DataCollatorWithPadding,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from datasets import DatasetDict\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import gensim.downloader\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from scipy.sparse import hstack as sparse_hstack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run dict_maker_lang.py "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'indonesian': Dataset({\n",
            "    features: ['question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url', 'question_words', 'answer_words', 'doc_words', 'doc_text_words', 'question_text_eng', 'doc_plaintext_eng', 'question_words_eng', 'doc_text_words_eng'],\n",
            "    num_rows: 11394\n",
            "}), 'bengali': Dataset({\n",
            "    features: ['question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url', 'question_words', 'answer_words', 'doc_words', 'doc_text_words', 'question_text_eng', 'doc_plaintext_eng', 'question_words_eng', 'doc_text_words_eng'],\n",
            "    num_rows: 4779\n",
            "}), 'arabic': Dataset({\n",
            "    features: ['question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url', 'question_words', 'answer_words', 'doc_words', 'doc_text_words', 'question_text_eng', 'doc_plaintext_eng', 'question_words_eng', 'doc_text_words_eng'],\n",
            "    num_rows: 29598\n",
            "})}\n"
          ]
        }
      ],
      "source": [
        "# If the .pkl files wont load, you need to run the dict_maker_lang.py file in the cell above. This file generates the translated dictionary.\n",
        "\n",
        "file_path = 'train_set_dict.pkl'\n",
        "with open(file_path, 'rb') as file:\n",
        "    loaded_data = pickle.load(file)\n",
        "train_set_dict = loaded_data['any']\n",
        "print(train_set_dict)\n",
        "\n",
        "file_path = 'val_set_dict.pkl'\n",
        "with open(file_path, 'rb') as file:\n",
        "    loaded_data = pickle.load(file)\n",
        "val_set_dict = loaded_data['any']\n",
        "print(val_set_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'indonesian': Dataset({\n",
            "    features: ['question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url', 'question_words', 'answer_words', 'doc_words', 'doc_text_words', 'question_text_eng', 'doc_plaintext_eng', 'question_words_eng', 'doc_text_words_eng'],\n",
            "    num_rows: 1191\n",
            "}), 'bengali': Dataset({\n",
            "    features: ['question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url', 'question_words', 'answer_words', 'doc_words', 'doc_text_words', 'question_text_eng', 'doc_plaintext_eng', 'question_words_eng', 'doc_text_words_eng'],\n",
            "    num_rows: 224\n",
            "}), 'arabic': Dataset({\n",
            "    features: ['question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url', 'question_words', 'answer_words', 'doc_words', 'doc_text_words', 'question_text_eng', 'doc_plaintext_eng', 'question_words_eng', 'doc_text_words_eng'],\n",
            "    num_rows: 1902\n",
            "})}\n"
          ]
        }
      ],
      "source": [
        "file_path = 'val_set_dict.pkl'\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    loaded_data = pickle.load(file)\n",
        "\n",
        "val_set_dict = loaded_data['any']\n",
        "\n",
        "print(val_set_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "languages = ['indonesian', 'bengali', 'arabic']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wld8Y_zWf0o-"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "foi0ntgKuu5J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl (12.8 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.3.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.2.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (61.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.21.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.64.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2021.10.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.0.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\kaspe\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tE5t7R7Puu5K"
      },
      "outputs": [],
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83yCQ2QZJf-0",
        "outputId": "c6a3cd77-330d-49e0-e002-dacb3614117f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sv_bCK3Gx7oR"
      },
      "outputs": [],
      "source": [
        "def run_classifier(X_train, y_train, X_test, y_test):\n",
        "  # Define hyperparams for search\n",
        "  # C = np.logspace(-6, 2, 50)\n",
        "  # warm_start = [False, True]\n",
        "  # class_weight = ['balanced', None]\n",
        "\n",
        "  # hp = {\"C\": C, \"warm_start\": warm_start, 'class_weight': class_weight}\n",
        "\n",
        "  classifier = LogisticRegression(penalty='l2', max_iter=1000)\n",
        "  # classifier_random = RandomizedSearchCV(\n",
        "  #     estimator=classifier,\n",
        "  #     param_distributions=hp,\n",
        "  #     n_iter=100,\n",
        "  #     cv=5,\n",
        "  #     verbose=2,\n",
        "  #     random_state=1000,\n",
        "  #     n_jobs=-1,\n",
        "  #     scoring='f1'\n",
        "  # )\n",
        "\n",
        "  # classifier_random.fit(X_train, y_train)\n",
        "  # print(classifier_random.best_params_)\n",
        "  # print(classifier_random.best_score_)\n",
        "  # model = classifier_random.best_estimator_\n",
        "  classifier.fit(X_train,y_train)\n",
        "  preds = classifier.predict(X_test)\n",
        "  print(classification_report(y_test, preds))\n",
        "\n",
        "def get_unigram_features(dataset, vectorizer):\n",
        "  X1 = vectorizer.transform(dataset[:,0])\n",
        "  X2 = vectorizer.transform(dataset[:,1])\n",
        "  X = sparse_hstack([X1, X2], format='csr')\n",
        "  y = list(dataset[:,2])\n",
        "  return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDInP4IImKGR",
        "outputId": "1876ac0a-4d29-488e-8502-83dd3f6f8ad2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "language:  indonesian\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.74      0.71       597\n",
            "           1       0.71      0.64      0.67       594\n",
            "\n",
            "    accuracy                           0.69      1191\n",
            "   macro avg       0.69      0.69      0.69      1191\n",
            "weighted avg       0.69      0.69      0.69      1191\n",
            "\n",
            "language:  bengali\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.70      0.66       112\n",
            "           1       0.66      0.60      0.63       112\n",
            "\n",
            "    accuracy                           0.65       224\n",
            "   macro avg       0.65      0.65      0.65       224\n",
            "weighted avg       0.65      0.65      0.65       224\n",
            "\n",
            "language:  arabic\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.79      0.77       956\n",
            "           1       0.77      0.73      0.75       946\n",
            "\n",
            "    accuracy                           0.76      1902\n",
            "   macro avg       0.76      0.76      0.76      1902\n",
            "weighted avg       0.76      0.76      0.76      1902\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\kaspe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "for lang in languages:\n",
        "  print('language: ', lang)\n",
        "  y_train = [0 if not answer else 1 for answer in train_set_dict[lang]['answer_words']]\n",
        "  y_val = [0 if not answer else 1 for answer in val_set_dict[lang]['answer_words']]\n",
        "\n",
        "  #yy = [int(value) for value in yy]\n",
        "  train_logistic = np.column_stack((train_set_dict[lang]['question_text_eng'], train_set_dict[lang]['doc_plaintext_eng'], y_train))\n",
        "\n",
        "  val_logistic = np.column_stack(([val_set_dict[lang]['question_text_eng'], val_set_dict[lang]['doc_plaintext_eng'], y_val]))\n",
        "  # Run the regression\n",
        "  vectorizer = CountVectorizer()\n",
        "  vectorizer.fit(train_logistic[:,0])\n",
        "  X_train,y_train = get_unigram_features(train_logistic, vectorizer)\n",
        "  X_val,y_val = get_unigram_features(val_logistic, vectorizer)\n",
        "  run_classifier(X_train, y_train, X_val, y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2pWYc_33LcT"
      },
      "source": [
        "### Good results for Logistic Regression shows that the data is linear seperable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwMZx8JpEL8r"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyE0nSRk2t0n",
        "outputId": "4c045068-385a-428c-e4f6-1313a5cd3c7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "language:  indonesian\n",
            "Accuracy on the validation set: 0.7397145256087322\n",
            "language:  bengali\n",
            "Accuracy on the validation set: 0.7455357142857143\n",
            "language:  arabic\n",
            "Accuracy on the validation set: 0.7760252365930599\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "for lang in languages:\n",
        "  print('language: ', lang)\n",
        "  y_train = [0 if not answer else 1 for answer in train_set_dict[lang]['answer_words']]\n",
        "  y_val = [0 if not answer else 1 for answer in val_set_dict[lang]['answer_words']]\n",
        "  train_logistic = np.column_stack((train_set_dict[lang]['question_text_eng'], train_set_dict[lang]['doc_plaintext_eng'], y_train))\n",
        "  val_logistic = np.column_stack(([val_set_dict[lang]['question_text_eng'], val_set_dict[lang]['doc_plaintext_eng'], y_val]))\n",
        "  # Run the regression\n",
        "  vectorizer = CountVectorizer()\n",
        "  vectorizer.fit(train_logistic[:,0])\n",
        "  X_train,y_train = get_unigram_features(train_logistic, vectorizer)\n",
        "  X_val,y_val = get_unigram_features(val_logistic, vectorizer)\n",
        "\n",
        "  rf_classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "  rf_classifier.fit(X_train, y_train)\n",
        "  y_val_pred = rf_classifier.predict(X_val)\n",
        "  accuracy = accuracy_score(y_val, y_val_pred)\n",
        "  print(f\"Accuracy on the validation set: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i72xVzH0EP5x"
      },
      "source": [
        "# Logistic Regression with Bpemb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjhJcfxzEOLG",
        "outputId": "75b4ce38-8ec8-471c-8424-f9ff4f7f5740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/id/id.wiki.bpe.vs25000.model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 650018/650018 [00:00<00:00, 6720534.75B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/id/id.wiki.bpe.vs25000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9465922/9465922 [00:00<00:00, 11037977.69B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/bn/bn.wiki.bpe.vs25000.model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 863227/863227 [00:00<00:00, 4736343.82B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/bn/bn.wiki.bpe.vs25000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9517491/9517491 [00:00<00:00, 10056916.51B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/ar/ar.wiki.bpe.vs25000.model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 742254/742254 [00:00<00:00, 4917327.82B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/ar/ar.wiki.bpe.vs25000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9491724/9491724 [00:00<00:00, 10723661.73B/s]\n"
          ]
        }
      ],
      "source": [
        "bpemb_id = BPEmb(lang='id', dim=100, vs=25000)\n",
        "bpemb_bn = BPEmb(lang='bn', dim=100, vs=25000)\n",
        "bpemb_ar = BPEmb(lang='ar', dim=100, vs=25000)\n",
        "\n",
        "bpemp_mapping = {\n",
        "    \"indonesian\": \"id\",\n",
        "    \"bengali\": \"bn\",\n",
        "    \"arabic\": \"ar\"\n",
        "  }\n",
        "\n",
        "\n",
        "def get_bpemb_features(dataset, bpemb):\n",
        "  X = [bpemb.embed(x).mean(0) for x in tqdm(dataset[:,0])]\n",
        "  y = list(dataset[:,1])\n",
        "  return X,y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptiOKwFn9DzH",
        "outputId": "4d9c2e67-14ad-48d8-d701-49bd5a088f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "language:  indonesian\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 11394/11394 [00:03<00:00, 3165.35it/s]\n",
            "100%|██████████| 1191/1191 [00:00<00:00, 3484.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.72      0.72       597\n",
            "           1       0.72      0.70      0.71       594\n",
            "\n",
            "    accuracy                           0.71      1191\n",
            "   macro avg       0.71      0.71      0.71      1191\n",
            "weighted avg       0.71      0.71      0.71      1191\n",
            "\n",
            "language:  bengali\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4779/4779 [00:01<00:00, 3067.24it/s]\n",
            "100%|██████████| 224/224 [00:00<00:00, 2787.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.63      0.68       112\n",
            "           1       0.68      0.77      0.72       112\n",
            "\n",
            "    accuracy                           0.70       224\n",
            "   macro avg       0.70      0.70      0.70       224\n",
            "weighted avg       0.70      0.70      0.70       224\n",
            "\n",
            "language:  arabic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29598/29598 [00:09<00:00, 2978.15it/s]\n",
            "100%|██████████| 1902/1902 [00:00<00:00, 3150.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.75      0.74       956\n",
            "           1       0.74      0.73      0.73       946\n",
            "\n",
            "    accuracy                           0.74      1902\n",
            "   macro avg       0.74      0.74      0.74      1902\n",
            "weighted avg       0.74      0.74      0.74      1902\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for lang in languages:\n",
        "    print('language: ', lang)\n",
        "    if lang in bpemp_mapping:\n",
        "        bpemb_model = globals()[f'bpemb_{bpemp_mapping[lang]}']  # Get the corresponding BPEmb model\n",
        "    else:\n",
        "        print(f\"No BPEmb model found for language: {lang}\")\n",
        "        continue\n",
        "\n",
        "    y_train = [0 if not answer else 1 for answer in train_set_dict[lang]['answer_words']]\n",
        "    y_val = [0 if not answer else 1 for answer in val_set_dict[lang]['answer_words']]\n",
        "\n",
        "    X_train = [item['question_text'] + ' [SEP] ' + item['document_plaintext'] for item in train_set_dict[lang]]\n",
        "    X_val = [item['question_text'] + ' [SEP] ' + item['document_plaintext'] for item in val_set_dict[lang]]\n",
        "\n",
        "    X_train, y_train = get_bpemb_features(np.transpose((X_train, y_train)), bpemb_model)\n",
        "    X_val, y_val = get_bpemb_features(np.transpose((X_val, y_val)), bpemb_model)\n",
        "    run_classifier(X_train, y_train, X_val, y_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Binary Classification, Transformer model\n",
        "The code below was inspired by https://huggingface.co/docs/transformers/tasks/sequence_classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you're using google colab, run the cell below and restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers datasets evaluate\n",
        "!pip install transformers[torch]\n",
        "!pip install datasets\n",
        "!pip install sklearn\n",
        "!pip install bertviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def enforce_reproducibility(seed=42):\n",
        "    # Sets seed manually for both CPU and CUDA\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # For atomic operations there is currently\n",
        "    # no simple way to enforce determinism, as\n",
        "    # the order of parallel operations is not known.\n",
        "    # CUDNN\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # System based\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding  indonesian to dict\n",
            "Adding  bengali to dict\n",
            "Adding  arabic to dict\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
        "train_set = dataset[\"train\"]\n",
        "validation_set = dataset[\"validation\"]\n",
        "\n",
        "languages = ['indonesian', 'bengali', 'arabic']\n",
        "\n",
        "train_set_dict = {}\n",
        "val_set_dict = {}\n",
        "\n",
        "for language in languages:\n",
        "    print(\"Adding \", language, \"to dict\")\n",
        "    train_set_dict[language] = train_set.filter(lambda example: example[\"language\"] == language)\n",
        "    val_set_dict[language] = validation_set.filter(lambda example: example[\"language\"] == language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Making sure that our dataset has the same shape as SquAD_V2\n",
        "\n",
        "def reformat_example(example, idx):\n",
        "    #example[\"id\"] = str(idx)\n",
        "\n",
        "    # Rename columns\n",
        "    example.pop(\"document_title\")\n",
        "    example[\"text\"] = example.pop(\"question_text\") + example.pop(\"document_plaintext\")\n",
        "    #example[\"question\"] =\n",
        "\n",
        "    example['label'] = int(len(example['annotations']['answer_text'][0]) > 0)\n",
        "\n",
        "    # Reformat the answers structure\n",
        "    example.pop(\"annotations\")\n",
        "\n",
        "    # example[\"answers\"] = {\n",
        "    #     \"text\": annotations[\"answer_text\"],\n",
        "    #     \"answer_start\": annotations[\"answer_start\"]\n",
        "    # }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return example\n",
        "\n",
        "TEST_SIZE = 0.15\n",
        "\n",
        "def training_split(train_dict, val_dict):\n",
        "    dict_list = DatasetDict()\n",
        "\n",
        "    for key in train_dict.keys():\n",
        "        hugging_dict = DatasetDict()\n",
        "\n",
        "        # Transform the train dataset\n",
        "        hugging_dict['train'] = train_dict[key].select_columns(['question_text', 'document_title', 'document_plaintext', 'annotations'])\n",
        "        hugging_dict['train'] = hugging_dict['train'].map(reformat_example, with_indices=True)\n",
        "\n",
        "        # Transform the validation dataset\n",
        "        hugging_dict['validation'] = val_dict[key].select_columns(['question_text', 'document_title', 'document_plaintext', 'annotations'])\n",
        "        hugging_dict['validation'] = hugging_dict['validation'].map(reformat_example, with_indices=True)\n",
        "\n",
        "        dict_list[key] = hugging_dict\n",
        "\n",
        "    return dict_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tydiqa = training_split(train_set_dict, val_set_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "id2label = {0: \"no_amswer\", 1: \"Answer\"}\n",
        "label2id = {\"no_answer\": 0, \"Answer\": 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Indonesian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "enforce_reproducibility()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_tydiqa_indo = tydiqa['indonesian'].map(preprocess_function, batched=True)\n",
        "tokenized_tydiqa_ber = tydiqa['bengali'].map(preprocess_function, batched=True)\n",
        "tokenized_tydiqa = tydiqa['arabic'].map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1426' max='1426' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1426/1426 08:07, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.367200</td>\n",
              "      <td>0.304980</td>\n",
              "      <td>0.877414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.216800</td>\n",
              "      <td>0.315467</td>\n",
              "      <td>0.887490</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"indonesian\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps = 100\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_tydiqa_indo[\"train\"],\n",
        "    eval_dataset=tokenized_tydiqa_indo[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"indonesian_classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: indonesian_classifier/ (stored 0%)\n",
            "  adding: indonesian_classifier/tokenizer.json (deflated 67%)\n",
            "  adding: indonesian_classifier/special_tokens_map.json (deflated 42%)\n",
            "  adding: indonesian_classifier/tokenizer_config.json (deflated 76%)\n",
            "  adding: indonesian_classifier/vocab.txt (deflated 45%)\n",
            "  adding: indonesian_classifier/pytorch_model.bin (deflated 7%)\n",
            "  adding: indonesian_classifier/training_args.bin (deflated 50%)\n",
            "  adding: indonesian_classifier/config.json (deflated 54%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_909265e2-6932-4e25-9b0d-b2d10baa9ab5\", \"indonesian_classifier.zip\", 661193491)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Zip the model directory\n",
        "!zip -r indonesian_classifier.zip indonesian_classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bengali"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4031687958845c0966db8d5fd56a44d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4779 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e287d0eb4324ded8d436bdbedc5d8d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/224 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_tydiqa_ber = tydiqa['bengali'].map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='598' max='598' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [598/598 05:18, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.475400</td>\n",
              "      <td>0.447014</td>\n",
              "      <td>0.799107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.290800</td>\n",
              "      <td>0.439516</td>\n",
              "      <td>0.825893</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"bengali_classifier\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps = 100\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_tydiqa[\"train\"],\n",
        "    eval_dataset=tokenized_tydiqa[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"bengali_classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BENGALI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: bengali_classifier/ (stored 0%)\n",
            "  adding: bengali_classifier/runs/ (stored 0%)\n",
            "  adding: bengali_classifier/runs/Nov01_08-28-23_e5a9b7b4a2c7/ (stored 0%)\n",
            "  adding: bengali_classifier/runs/Nov01_08-28-23_e5a9b7b4a2c7/events.out.tfevents.1698827305.e5a9b7b4a2c7.12314.0 (deflated 58%)\n",
            "  adding: bengali_classifier/checkpoint-299/ (stored 0%)\n",
            "  adding: bengali_classifier/checkpoint-299/rng_state.pth (deflated 25%)\n",
            "  adding: bengali_classifier/checkpoint-299/scheduler.pt (deflated 56%)\n",
            "  adding: bengali_classifier/checkpoint-299/tokenizer.json (deflated 67%)\n",
            "  adding: bengali_classifier/checkpoint-299/special_tokens_map.json (deflated 42%)\n",
            "  adding: bengali_classifier/checkpoint-299/tokenizer_config.json (deflated 76%)\n",
            "  adding: bengali_classifier/checkpoint-299/vocab.txt (deflated 45%)\n",
            "  adding: bengali_classifier/checkpoint-299/optimizer.pt (deflated 53%)\n",
            "  adding: bengali_classifier/checkpoint-299/pytorch_model.bin (deflated 7%)\n",
            "  adding: bengali_classifier/checkpoint-299/trainer_state.json (deflated 56%)\n",
            "  adding: bengali_classifier/checkpoint-299/training_args.bin (deflated 51%)\n",
            "  adding: bengali_classifier/checkpoint-299/config.json (deflated 54%)\n",
            "  adding: bengali_classifier/tokenizer.json (deflated 67%)\n",
            "  adding: bengali_classifier/special_tokens_map.json (deflated 42%)\n",
            "  adding: bengali_classifier/tokenizer_config.json (deflated 76%)\n",
            "  adding: bengali_classifier/vocab.txt (deflated 45%)\n",
            "  adding: bengali_classifier/pytorch_model.bin (deflated 7%)\n",
            "  adding: bengali_classifier/training_args.bin (deflated 51%)\n",
            "  adding: bengali_classifier/checkpoint-598/ (stored 0%)\n",
            "  adding: bengali_classifier/checkpoint-598/rng_state.pth (deflated 25%)\n",
            "  adding: bengali_classifier/checkpoint-598/scheduler.pt (deflated 56%)\n",
            "  adding: bengali_classifier/checkpoint-598/tokenizer.json (deflated 67%)\n",
            "  adding: bengali_classifier/checkpoint-598/special_tokens_map.json (deflated 42%)\n",
            "  adding: bengali_classifier/checkpoint-598/tokenizer_config.json (deflated 76%)\n",
            "  adding: bengali_classifier/checkpoint-598/vocab.txt (deflated 45%)\n",
            "  adding: bengali_classifier/checkpoint-598/optimizer.pt (deflated 53%)\n",
            "  adding: bengali_classifier/checkpoint-598/pytorch_model.bin (deflated 7%)\n",
            "  adding: bengali_classifier/checkpoint-598/trainer_state.json (deflated 66%)\n",
            "  adding: bengali_classifier/checkpoint-598/training_args.bin (deflated 51%)\n",
            "  adding: bengali_classifier/checkpoint-598/config.json (deflated 54%)\n",
            "  adding: bengali_classifier/config.json (deflated 54%)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Zip the model directory\n",
        "!zip -r bengali_classifier.zip bengali_classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "enforce_reproducibility()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_tydiqa = tydiqa['arabic'].map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3700' max='3700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3700/3700 26:32, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.231300</td>\n",
              "      <td>0.238447</td>\n",
              "      <td>0.915878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.176200</td>\n",
              "      <td>0.229928</td>\n",
              "      <td>0.932177</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"arabic_classifier\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps = 100\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_tydiqa[\"train\"],\n",
        "    eval_dataset=tokenized_tydiqa[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"arabic_classifier_final\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
