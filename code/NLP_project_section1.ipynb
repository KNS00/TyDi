{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TjjvXNaWGSWA"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install datasets\n",
    "!pip3 install googletrans==3.1.0a0\n",
    "!pip3 install torch\n",
    "!pip3 install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yR-SFZwMv4M-",
    "outputId": "b294e341-9637-4fa1-aaa6-6b95f95a43c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marcusthomsen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marcusthomsen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "from datasets import load_dataset\n",
    "from tabulate import tabulate\n",
    "from googletrans import Translator\n",
    "\n",
    "# stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Myx8mt0HYfBu"
   },
   "source": [
    "# Week 36 (5-11 September)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryW0aPvra6oD"
   },
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YjiUjRO4qw2"
   },
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marcusthomsen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marcusthomsen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Success\n",
      "Adding  indonesian to dict\n",
      "Adding  bengali to dict\n",
      "Adding  arabic to dict\n",
      "\n",
      "Tokenizing question, answer and document text for indonesian\n",
      "Done\n",
      "\n",
      "\n",
      "Tokenizing question, answer and document text for bengali\n",
      "Done\n",
      "\n",
      "\n",
      "Tokenizing question, answer and document text for arabic\n",
      "Done\n",
      "\n",
      "Removing stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29598/29598 [01:30<00:00, 325.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "languages = ['indonesian', 'bengali', 'arabic']\n",
    "%run dict_maker.py\n",
    "train_set_dict = train_set_dict\n",
    "val_set_dict = val_set_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWzC09A746Pv"
   },
   "source": [
    "### Aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "G78O6v_Ejyes"
   },
   "outputs": [],
   "source": [
    "#Creating BOW\n",
    "def unique_words(data):\n",
    "  word_to_ix = {}\n",
    "  for sent in data:\n",
    "      for word in sent:\n",
    "          if word not in word_to_ix:\n",
    "              word_to_ix[word] = len(word_to_ix)\n",
    "  return word_to_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp6bmnIbceUE"
   },
   "source": [
    "## Basic statistics: no. of questions avg words per question and no. of answerable questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gnU7YUCyhbR",
    "outputId": "a24e921a-bfad-4097-b364-b8c209aeb0e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrr}\n",
      "\\hline\n",
      " Language   & Dataset        &   No. of Questions &   Avg. Words per Question &   Answerable Ratio &   No. of Words in Questions &   No. of Unique Words \\\\\n",
      "\\hline\n",
      " indonesian & Train Set      &              11394 &                      3.37 &                0.5 &                       38374 &                  5558 \\\\\n",
      "            & Validation Set &               1191 &                      3.62 &                0.5 &                        4306 &                  1285 \\\\\n",
      " bengali    & Train Set      &               4779 &                      7.04 &                0.5 &                       33623 &                  3744 \\\\\n",
      "            & Validation Set &                224 &                      7.43 &                0.5 &                        1664 &                   431 \\\\\n",
      " arabic     & Train Set      &              29598 &                      4.12 &                0.5 &                      121969 &                 16183 \\\\\n",
      "            & Validation Set &               1902 &                      3.97 &                0.5 &                        7556 &                  2208 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "table_data = []\n",
    "for language in languages:\n",
    "    train_set = train_set_dict[language]\n",
    "    val_set = val_set_dict[language]\n",
    "\n",
    "    train_questions = train_set['question_words']\n",
    "    val_questions = val_set['question_words']\n",
    "\n",
    "    stop_words = set(stopwords.words(language.lower()))\n",
    "\n",
    "    train_word_counts = len(train_questions)\n",
    "    val_word_counts = len(val_questions)\n",
    "    train_answerable_ratio = np.mean([len(x['annotations']['answer_text'][0]) > 0 for x in train_set])\n",
    "    val_answerable_ratio = np.mean([len(nltk.word_tokenize(x['annotations']['answer_text'][0])) > 0 for x in val_set])\n",
    "\n",
    "    flat_train_q = [item for sublist in train_questions for item in sublist]\n",
    "    flat_val_q = [item for sublist in val_questions for item in sublist]\n",
    "\n",
    "    # Calculate the total length of inner lists\n",
    "    train_avg_words_per_question = sum(len(inner_list) for inner_list in train_questions) / len(train_questions)\n",
    "    val_avg_words_per_question = sum(len(inner_list) for inner_list in val_questions) / len(val_questions)\n",
    "\n",
    "    train_total_words = train_word_counts * train_avg_words_per_question\n",
    "    val_total_words = val_word_counts * val_avg_words_per_question\n",
    "\n",
    "    train_unique_words = len(unique_words(train_questions))\n",
    "    val_unique_words = len(unique_words(val_questions))\n",
    "\n",
    "    table_data.append([\n",
    "        language,\n",
    "        \"Train Set\",\n",
    "        len(train_questions),\n",
    "        np.round(train_avg_words_per_question, 2),\n",
    "        np.round(train_answerable_ratio, 2),\n",
    "        np.round(train_total_words, 2),\n",
    "        np.round(train_unique_words, 2)\n",
    "    ])\n",
    "    table_data.append([\n",
    "        \"\",\n",
    "        \"Validation Set\",\n",
    "        len(val_questions),\n",
    "        np.round(val_avg_words_per_question, 2),\n",
    "        np.round(val_answerable_ratio, 2),\n",
    "        np.round(val_total_words, 2),\n",
    "        np.round(val_unique_words, 2)\n",
    "    ])\n",
    "\n",
    "headers = [\"Language\", \"Dataset\", \"No. of Questions\", \"Avg. Words per Question\", \"Answerable Ratio\", \"No. of Words in Questions\", \"No. of Unique Words\"]\n",
    "\n",
    "table = tabulate(table_data, headers, tablefmt=\"latex_raw\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNSJfdKzSazQ"
   },
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IL4IHoSJSxk6",
    "outputId": "92ea323e-bc16-4a3e-dc4f-dae3766e9978"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marcusthomsen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marcusthomsen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marcusthomsen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Language      Type Top Word (English)  Count\n",
      "0       Arabic       doc     classification  15655\n",
      "1       Arabic       doc            general  13640\n",
      "2       Arabic       doc                  1   8364\n",
      "3       Arabic       doc                son   6161\n",
      "4       Arabic       doc                  a   5842\n",
      "5       Arabic  question              child   1564\n",
      "6       Arabic  question            Located   1540\n",
      "7       Arabic  question   It was completed   1334\n",
      "8       Arabic  question            general   1159\n",
      "9       Arabic  question               city   1002\n",
      "10     Bengali       doc                 is   2916\n",
      "11     Bengali       doc                 in   2365\n",
      "12     Bengali       doc                 do   2355\n",
      "13     Bengali       doc                 as   1334\n",
      "14     Bengali       doc           by doing   1293\n",
      "15     Bengali  question               name    837\n",
      "16     Bengali  question                 in    522\n",
      "17     Bengali  question              where    300\n",
      "18     Bengali  question         Bangladesh    266\n",
      "19     Bengali  question               born    252\n",
      "20  Indonesian       doc          Indonesia   2371\n",
      "21  Indonesian       doc                  1   2370\n",
      "22  Indonesian       doc                own   1802\n",
      "23  Indonesian       doc            country   1702\n",
      "24  Indonesian       doc                  2   1679\n",
      "25  Indonesian  question               wide    858\n",
      "26  Indonesian  question               Name    736\n",
      "27  Indonesian  question               time    538\n",
      "28  Indonesian  question              Where    508\n",
      "29  Indonesian  question          Indonesia    470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_top_words(countries: list):\n",
    "    combined_data = {'Language': [], 'Type': [], 'Top Word (English)': [], 'Count': []}\n",
    "    for country in countries:\n",
    "        nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words(country.lower()))\n",
    "\n",
    "        # we decided to remove additional characters that were top words\n",
    "        # for the countries because they are not particulary interesting\n",
    "        additional_characters = list(string.punctuation) + [\"``\", \"''\", \"؟\", \",\"]\n",
    "\n",
    "        doc_words = [word for sublist in train_set_dict[country]['doc_text_words'] for word in sublist if word not in stop_words and word not in additional_characters]\n",
    "        question_words = [word for sublist in train_set_dict[country]['question_words'] for word in sublist if word not in stop_words and word not in additional_characters]\n",
    "\n",
    "        top_words_doc = pd.Series(doc_words).value_counts()[:5]\n",
    "        top_words_q = pd.Series(question_words).value_counts()[:5]\n",
    "\n",
    "        translator = Translator()\n",
    "\n",
    "        translated_words_doc = [translator.translate(word, src=country, dest='en').text for word in top_words_doc.index]\n",
    "        translated_words_q = [translator.translate(word, src=country, dest='en').text for word in top_words_q.index]\n",
    "\n",
    "        for word, count in zip(translated_words_doc, top_words_doc.values):\n",
    "            combined_data['Language'].append(country.capitalize())\n",
    "            combined_data['Type'].append('doc')\n",
    "            combined_data['Top Word (English)'].append(word)\n",
    "            combined_data['Count'].append(count)\n",
    "\n",
    "        for word, count in zip(translated_words_q, top_words_q.values):\n",
    "            combined_data['Language'].append(country.capitalize())\n",
    "            combined_data['Type'].append('question')\n",
    "            combined_data['Top Word (English)'].append(word)\n",
    "            combined_data['Count'].append(count)\n",
    "\n",
    "    combined_df = pd.DataFrame(combined_data)\n",
    "    print(combined_df)\n",
    "countries = ['arabic', 'bengali', 'indonesian']\n",
    "\n",
    "table = get_top_words(countries)\n",
    "\n",
    "print(tabulate(table, headers='keys', tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KF2v92H4JUA"
   },
   "source": [
    "## (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_9_f3ikQdZV9"
   },
   "outputs": [],
   "source": [
    "first_column = [item['answer_text'][0] for item in train_set_dict['indonesian']['annotations']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Gvvb13CTdZY2"
   },
   "outputs": [],
   "source": [
    "def oracle(data, language):\n",
    "  labels = []\n",
    "  for elem in data[language]:\n",
    "      if elem['annotations']['answer_text'][0] == '':\n",
    "          labels.append(0)\n",
    "      else:\n",
    "          labels.append(1)\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YmsrFmQS1187"
   },
   "outputs": [],
   "source": [
    "# Function that returns 1 if more than or equal to n words from questions are in plaintext\n",
    "def is_answerable(question_words, doc_words, n):\n",
    "    common_words = [word for word in question_words if word in doc_words]\n",
    "    return len(common_words) >= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8oUZ4k-wFbEM"
   },
   "outputs": [],
   "source": [
    "new_data = {}\n",
    "def rule_based_classifier(data, country, n):\n",
    "    new_data = copy.deepcopy(data[country]) \n",
    "\n",
    "    answerable_values = []\n",
    "\n",
    "    for i in range(0,data[country].num_rows):\n",
    "        q = set(new_data['question_words'][i])\n",
    "        doc = new_data['doc_text_words'][i]\n",
    "        answerable = 0  \n",
    "\n",
    "        answerable = is_answerable(q, doc, n)\n",
    "\n",
    "        answerable_values.append(answerable)\n",
    "    new_data\n",
    "    #new_data['answerable'] = answerable_values\n",
    "\n",
    "    return answerable_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0h4tM3UNtfiQ",
    "outputId": "0738f77e-1a56-4109-d2e0-c192c3810b56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of our classifier for indonesian\n",
    "indo_classifier_answers = rule_based_classifier(val_set_dict, 'indonesian', 2)\n",
    "np.round((np.array(indo_classifier_answers) == np.array(oracle(val_set_dict, \"indonesian\"))).mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQuam-c4t15d",
    "outputId": "7df4ccc4-d840-41fc-9184-71697be21ea0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of our classifier for bengali\n",
    "beng_classifier_answers = rule_based_classifier(val_set_dict, 'bengali', 2)\n",
    "np.round((np.array(beng_classifier_answers) == np.array(oracle(val_set_dict, 'bengali'))).mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggjbaH_2t7Ch",
    "outputId": "0d51a72b-b3c6-4304-ddc8-79d8c9bc8365"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of our classifier for arabic\n",
    "arab_classifier_answers = rule_based_classifier(val_set_dict, 'arabic', 2)\n",
    "np.round((np.array(arab_classifier_answers) == np.array(oracle(val_set_dict, 'arabic'))).mean(), 1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
