{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classifiers\n",
    "\n",
    "This section aims to implement models that predict whether an answer to a quetsion is within a document or not. \n",
    "\n",
    "3 models are implemented: two logistic regression models with GloVe and BPEmb embeddings, respectively, and a neural network which utilizies BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding  english to dict\n"
     ]
    }
   ],
   "source": [
    "%run /app/prepare_data.py english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with BPEmb embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing an example of the model for the word hello:\n",
      "\n",
      "Subword tokens: ['▁hel', 'lo']\n",
      "Values: [[ 0.101289  0.067421  0.445592 -0.31668   0.210379 -0.102614  0.329681\n",
      "   0.3211    0.045224  1.001417 -0.173007  0.146657  0.431487  0.265748\n",
      "   0.367141  0.022575 -0.531671 -0.071386  0.141523 -0.026373  0.077574\n",
      "   0.329998 -0.250906  0.071113 -0.104565  0.209173 -0.340813  0.079831\n",
      "  -0.293946 -0.055277  0.375396  0.030002 -0.093638  0.300789 -0.793242\n",
      "   0.253135  0.018888  0.031131  0.574456  0.155547  0.040618 -0.023185\n",
      "   0.596075 -0.035391 -0.431783  0.649094 -0.141042  0.618206  0.156273\n",
      "  -0.257083]\n",
      " [-0.104277 -0.03539   0.248758 -0.41444   0.476999  0.258883  0.035058\n",
      "   0.273445 -0.342508  0.628149 -0.196841 -0.167787  0.058616 -0.096752\n",
      "   0.11608  -0.638848 -0.09259  -0.081885 -0.453536  0.233586  0.508515\n",
      "   0.261741 -0.00718   0.31664   0.021489 -0.263001 -0.267236  0.040581\n",
      "  -0.72942   0.035982  0.260991  0.577699  0.028912  0.001675  0.082996\n",
      "   0.405256 -0.345392 -0.188424 -0.013326 -0.063085 -0.064586  0.122327\n",
      "   0.182105 -0.181754  0.394809  0.822232  0.121792  0.321524 -0.25007\n",
      "   0.077297]]\n",
      "\n",
      "The model is divided into two subwords, 'he' and 'llo' - each with different values.\n",
      "The value of the subword depends on the context.\n"
     ]
    }
   ],
   "source": [
    "bpemb_model = BPEmb(lang='en', dim=50, vs=25000) # change language here if needed\n",
    "print(f\"Showing an example of the model for the word hello:\\n\")\n",
    "print(f\"Subword tokens: {bpemb_model.encode('hello')}\")\n",
    "print(f\"Values: {bpemb_model.embed('hello')}\\n\")\n",
    "print(\"The model is divided into two subwords, 'he' and 'llo' - each with different values.\\nThe value of the subword depends on the context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  vector representation for each text; we decided to average the vector embeddings of each word\n",
    "def get_bpemb_features(dataset, bpemb):\n",
    "    X = [bpemb.embed(\" \".join(x)).mean(0) for x in tqdm(dataset[:, 0])]\n",
    "    y = list(dataset[:, 1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a logistic regression model for the english language...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kasperschiller/opt/anaconda3/envs/bpemb_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n",
      "100%|██████████| 7389/7389 [00:04<00:00, 1769.22it/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 1697.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 15 words of the first text in the training set for english: \n",
      " ['When', 'was', 'quantum', 'field', 'theory', 'developed', '?', ' [SEP] ', 'Quantum', 'field', 'theory', 'naturally', 'began', 'with', 'the']...\n",
      "The vector representation of that text: \n",
      " [-0.12067941 -0.4207224   0.1596494   0.12411743 -0.26608768  0.05689759\n",
      "  0.12732057 -0.33251977  0.17155759  0.03208057 -0.14067082  0.04167471\n",
      "  0.25372058  0.28912488  0.05295061] ...\n",
      "\n",
      "Classification report for the english language:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.63      0.66       495\n",
      "           1       0.66      0.71      0.68       495\n",
      "\n",
      "    accuracy                           0.67       990\n",
      "   macro avg       0.67      0.67      0.67       990\n",
      "weighted avg       0.67      0.67      0.67       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lang in languages:\n",
    "    print(f\"Training a logistic regression model for the {lang} language...\")\n",
    "    X_train = [item['doc_words'] for item in train_set_dict[lang]]\n",
    "    X_val = [item['doc_words'] for item in val_set_dict[lang]]\n",
    "    y_train = [item['answerable'] for item in train_set_dict[lang]]\n",
    "    y_val = [item['answerable'] for item in val_set_dict[lang]]\n",
    "    X_train_bpemb, y_train_bpemb = get_bpemb_features(np.transpose((X_train, y_train)), bpemb_model)\n",
    "    X_val_bpemb, y_val_bpemb = get_bpemb_features(np.transpose((X_val, y_val)), bpemb_model)\n",
    "    print(f\"The first 15 words of the first text in the training set for {lang}: \\n {X_train[0][:15]}...\")\n",
    "    print(f\"The vector representation of that text: \\n {X_train_bpemb[0][:15]} ...\\n\")\n",
    "    classifier = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "    classifier.fit(X_train_bpemb, y_train_bpemb)\n",
    "    preds = classifier.predict(X_val_bpemb)\n",
    "    print(f\"Classification report for the {lang} language:\")\n",
    "    print(classification_report(y_val_bpemb, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_features(X, vectorizer):\n",
    "  X1 = vectorizer.transform(X[0])\n",
    "  X2 = vectorizer.transform(X[1])\n",
    "  X_ = sparse_hstack([X1, X2], format='csr')\n",
    "  return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with GloVe embeddings\n",
    "\n",
    "This embedding is only for the english language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import hstack as sparse_hstack\n",
    "import gensim.downloader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing an example of the model for the word hello:\n",
      "\n",
      "Embedding for 'hello': [ 0.26688    0.39632    0.6169    -0.77451   -0.1039     0.26697\n",
      "  0.2788     0.30992    0.0054685 -0.085256   0.73602   -0.098432\n",
      "  0.5479    -0.030305   0.33479    0.14094   -0.0070003  0.32569\n",
      "  0.22902    0.46557   -0.19531    0.37491   -0.7139    -0.51775\n",
      "  0.77039    1.0881    -0.66011   -0.16234    0.9119     0.21046\n",
      "  0.047494   1.0019     1.1133     0.70094   -0.08696    0.47571\n",
      "  0.1636    -0.44469    0.4469    -0.93817    0.013101   0.085964\n",
      " -0.67456    0.49662   -0.037827  -0.11038   -0.28612    0.074606\n",
      " -0.31527   -0.093774  -0.57069    0.66865    0.45307   -0.34154\n",
      " -0.7166    -0.75273    0.075212   0.57903   -0.1191    -0.11379\n",
      " -0.10026    0.71341   -1.1574    -0.74026    0.40452    0.18023\n",
      "  0.21449    0.37638    0.11239   -0.53639   -0.025092   0.31886\n",
      " -0.25013   -0.63283   -0.011843   1.377      0.86013    0.20476\n",
      " -0.36815   -0.68874    0.53512   -0.46556    0.27389    0.4118\n",
      " -0.854     -0.046288   0.11304   -0.27326    0.15636   -0.20334\n",
      "  0.53586    0.59784    0.60469    0.13735    0.42232   -0.61279\n",
      " -0.38486    0.35842   -0.48464    0.30728  ]\n",
      "The value still depends on context, but it is not divided into subwords.\n"
     ]
    }
   ],
   "source": [
    "hello_vector = glove_vectors[\"hello\"]\n",
    "print(f\"Showing an example of the model for the word hello:\\n\")\n",
    "print(f\"Embedding for 'hello': {hello_vector}\")\n",
    "\n",
    "print(f\"The value still depends on context, but it is not divided into subwords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_embeddings(texts, glove_vectors):\n",
    "    \"\"\"\n",
    "    Converts a list of texts into sequences of word embeddings.\n",
    "    Handles sequences of arbitrary lengths without truncation.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        words = text.split()  # Tokenize text\n",
    "        word_embeddings = [\n",
    "            glove_vectors[word] for word in words if word in glove_vectors\n",
    "        ]\n",
    "        if not word_embeddings:\n",
    "            # Default to a single zero vector for empty texts\n",
    "            word_embeddings = [np.zeros(100)]\n",
    "        embeddings.append(np.array(word_embeddings))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a logistic regression model for the english language...\n",
      "The first text in the training set for english: \n",
      " When was quantum field theory developed? Quantum field theory naturally began with the study of electromagnetic interactions, as the electromagnetic field was the only known classical field as of the 1920s.[8]:1\n",
      "The GloVe representation (averaged): \n",
      " [-0.06727608  0.22719015  0.21718472  0.13494685  0.15845731  0.03247473\n",
      "  0.15612006 -0.06552999 -0.6081013   0.15663871  0.08369958 -0.18789771\n",
      "  0.1966625   0.07496168  0.26428124 -0.26750705  0.2341432   0.2525144\n",
      " -0.015551   -0.19929105  0.12247273 -0.2077063   0.35634932  0.01751207\n",
      "  0.23028867 -0.24267037  0.04861677 -0.27640134  0.13263808  0.17019576\n",
      " -0.32764196  0.37578768 -0.09675872  0.19454098 -0.07862025  0.00471331\n",
      " -0.06276357  0.6262489  -0.07622276 -0.15160385 -0.48151222 -0.13967013\n",
      " -0.04628307 -0.18813136 -0.03559667 -0.00297623  0.32025963 -0.03209976\n",
      " -0.16816299 -0.37845576  0.1398484   0.01465227  0.20922127  0.96015096\n",
      " -0.09964465 -2.2103658  -0.04268016 -0.25056335  1.4478285   0.6620897\n",
      " -0.06841366  0.6422777  -0.03772711  0.3529449   0.9644352  -0.0934345\n",
      "  0.33794546 -0.07323488  0.21032453  0.0789869  -0.10027672 -0.1798552\n",
      "  0.12937015 -0.19093004  0.09461616 -0.16383678  0.19240801 -0.20629096\n",
      " -0.9116735  -0.00497515  0.51029944  0.1956021  -0.6263249   0.02595806\n",
      " -1.2417276   0.04982227  0.164516   -0.4309955   0.08199246 -0.40592715\n",
      " -0.09632738 -0.26880306 -0.43010423  0.33893156 -0.186219   -0.09525689\n",
      " -0.5269426  -0.452299    0.39994112  0.13181424] \n",
      "\n",
      "Classification report for the english language:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.64      0.65       495\n",
      "           1       0.65      0.66      0.65       495\n",
      "\n",
      "    accuracy                           0.65       990\n",
      "   macro avg       0.65      0.65      0.65       990\n",
      "weighted avg       0.65      0.65      0.65       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lang in languages:\n",
    "    print(f\"Training a logistic regression model for the {lang} language...\")\n",
    "    \n",
    "    # Combine question and document text for training and validation\n",
    "    X_train = [\n",
    "        f\"{q} {d}\" for q, d in zip(train_set_dict[lang]['question_text'], train_set_dict[lang]['document_plaintext'])\n",
    "    ]\n",
    "    X_val = [\n",
    "        f\"{q} {d}\" for q, d in zip(val_set_dict[lang]['question_text'], val_set_dict[lang]['document_plaintext'])\n",
    "    ]\n",
    "    \n",
    "    y_train = [item['answerable'] for item in train_set_dict[lang]]\n",
    "    y_val = [item['answerable'] for item in val_set_dict[lang]]\n",
    "    # Convert texts to sequences of concatenated word embeddings\n",
    "    X_train_embedded = text_to_word_embeddings(X_train, glove_vectors)\n",
    "    X_val_embedded = text_to_word_embeddings(X_val, glove_vectors)\n",
    "    \n",
    "    # Logistic regression requires fixed input size\n",
    "    # Use average pooling as a simple solution\n",
    "    X_train_fixed = np.array([np.mean(x.reshape(-1, 100), axis=0) for x in X_train_embedded])\n",
    "    X_val_fixed = np.array([np.mean(x.reshape(-1, 100), axis=0) for x in X_val_embedded])\n",
    "    \n",
    "    print(f\"The first text in the training set for {lang}: \\n {X_train[0]}\")\n",
    "    print(f\"The GloVe representation (averaged): \\n {X_train_fixed[0]} \\n\")\n",
    "    \n",
    "    # Train logistic regression classifier\n",
    "    classifier = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "    classifier.fit(X_train_fixed, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    preds = classifier.predict(X_val_fixed)\n",
    "    report = classification_report(y_val, preds)\n",
    "    print(f\"Classification report for the {lang} language:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing language: english\n",
      "Validation set size for english: 891\n",
      "Test set size for english: 99\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-multilingual-cased\"\n",
    "val_set_dict_, test_set_dict_ = separate_val_set(val_set_dict, languages, test_size=0.1, random_state=42)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing an example of tokenizing from BERT for the word hello:\n",
      "\n",
      "Tokens: ['hell', '##o']\n",
      "Encoded IDs: tensor([[  101, 61694, 10133,   102]])\n",
      "Token Sequence: ['[CLS]', 'hell', '##o', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Showing an example of tokenizing from BERT for the word hello:\\n\")\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokens = tokenizer.tokenize(\"hello\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "encoded = tokenizer(\"hello\", return_tensors=\"pt\")\n",
    "print(f\"Encoded IDs: {encoded['input_ids']}\")\n",
    "decoded = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "print(f\"Token Sequence: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"no_answer\", 1: \"answer\"}\n",
    "label2id = {\"no_answer\": 0, \"answer\": 1}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0723a6c56c4243906e96d33d8ddfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca2d85b772e4e48b2f219cb44bd19d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c621e013ed4340aba5f46cdb0acb0066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddc62890fae4c15aae8968f1e7e4113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0d23d2f0f2442b9399474dae74d79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bert_reformat(example):\n",
    "    \"\"\"\n",
    "    Match the format of the data to the BERT model.\n",
    "    \"\"\"\n",
    "    example.pop(\"document_title\")\n",
    "    example[\"text\"] = example.pop(\"question_text\") + example.pop(\"document_plaintext\")\n",
    "    example['label'] = int(len(example['annotations']['answer_text'][0]) > 0)\n",
    "    example.pop(\"annotations\")\n",
    "    return example\n",
    "\n",
    "\n",
    "def bert_prepare(train_dict, val_dict, test_dict):\n",
    "    \"\"\"\n",
    "    Prepares datasets for BERT by applying reformatting and selecting columns.\n",
    "\n",
    "    Parameters:\n",
    "        train_dict (dict): Dictionary containing train datasets by language.\n",
    "        val_dict (dict): Dictionary containing validation datasets by language.\n",
    "        test_dict (dict): Dictionary containing test datasets by language.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of DatasetDicts for each language.\n",
    "    \"\"\"\n",
    "    def filter_columns(dataset, required_columns):\n",
    "        # helper function to select columns\n",
    "        return dataset.map(lambda x: {key: x[key] for key in required_columns})\n",
    "\n",
    "    dict_list = {}\n",
    "    for key in train_dict.keys():\n",
    "        hugging_dict = DatasetDict()\n",
    "\n",
    "        # reformatting the validation and test datasets: train is already formatted correctly since we didn't split it\n",
    "        val_dataset = Dataset.from_dict(val_dict[key])\n",
    "        test_dataset = Dataset.from_dict(test_dict[key])\n",
    "\n",
    "        # Reformat and transform the train dataset\n",
    "        hugging_dict['train'] = train_dict[key].map(bert_reformat)\n",
    "\n",
    "        # Reformat and transform the validation dataset\n",
    "        hugging_dict['validation'] = val_dataset.map(bert_reformat)\n",
    "\n",
    "        # Reformat and transform the test dataset\n",
    "        hugging_dict['test'] = test_dataset.map(bert_reformat)\n",
    "\n",
    "        dict_list[key] = hugging_dict\n",
    "\n",
    "    return dict_list\n",
    "def tokenizer_bert(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=512)\n",
    "bert_data = bert_prepare(train_set_dict, val_set_dict_, test_set_dict_)\n",
    "tokenized_bert_data = bert_data['english'].map(tokenizer_bert, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-25-6a1b4b632d58>:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='924' max='924' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [924/924 17:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.327685</td>\n",
       "      <td>0.846240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>0.338619</td>\n",
       "      <td>0.867565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_score(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"english\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_bert_data['train'],\n",
    "    eval_dataset=tokenized_bert_data['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"english_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1\n",
      " 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1\n",
      " 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1]\n",
      "True labels: [1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_bert_data[\"test\"])\n",
    "predicted_labels = predictions.predictions.argmax(-1)\n",
    "print(f\"Predicted labels: {predicted_labels[:100]}\")\n",
    "print(f\"True labels: {tokenized_bert_data['test']['label'][:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8181818181818182\n",
      "F1 score: 0.8043478260869565\n"
     ]
    }
   ],
   "source": [
    "true_labels = tokenized_bert_data[\"test\"][\"label\"]\n",
    "acc = accuracy_score(y_true=predicted_labels, y_pred=true_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"F1 score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
