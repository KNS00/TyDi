{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This section aims to provide simple statistics for the TyDi data set as a starter for the project. A clear explanation of results can be found in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print(\"Using GPU\")\n",
    "else:\n",
    "  print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "The script below creates the dictionaries test_set_dict and val_set_dict for each language given as input. It is used in every section in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c842204a4948ee961aab7bb00e6e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b920bb89b8544e39e1504e5525a1341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38123ad029364b46a34fb2599c9e74b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-af2f3eaa87d1aa8b.parquet:   0%|          | 0.00/71.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b34863d5424d9aaa001dc72c88a184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-1f04eb244a33fa1b.parquet:   0%|          | 0.00/7.49M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea35122e4cb44119011ef911b60e76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/116067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33300d55a6cd452ba88a6352b659151f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13325 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding  english to dict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a36064d6b8d4570bb73d79a2ce32b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/116067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187c3da99e8945d3b0f51aae0720e93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/13325 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9831c372ce4590bc8eed095159b033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbf3cb5953941398114fc1ce3bdfd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run /app/prepare_data.py english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'english': Dataset({\n",
       "      features: ['question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url', 'question_words', 'answer_words', 'doc_words', 'doc_text_words', 'answerable'],\n",
       "      num_rows: 7389\n",
       "  })},\n",
       " {'english': Dataset({\n",
       "      features: ['question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url', 'question_words', 'answer_words', 'doc_words', 'doc_text_words', 'answerable'],\n",
       "      num_rows: 990\n",
       "  })},\n",
       " ['english'])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#showcasing the format of the data sets\n",
    "train_set_dict, val_set_dict, languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When was quantum field theory developed?\n",
      "Quantum field theory naturally began with the study of electromagnetic interactions, as the electromagnetic field was the only known classical field as of the 1920s.[8]:1\n",
      "{'answer_start': [159], 'answer_text': ['1920s']}\n",
      "\n",
      "\n",
      "What is the population of Mahwah, NJ?\n",
      "The previous mayor, Bill Laforet faced a recall election in November 2018, after a resident group submitted in June a list of 5,000 petition signatures that they had collected calling for the action, in excess of the 25% needed to place the measure in front of voters.[85] In the November 2018 general election, Laforet was recalled from office and John Roth was elected mayor. The successful recall was the first in the county for at least 25 years.[86]\n",
      "{'answer_start': [-1], 'answer_text': ['']}\n"
     ]
    }
   ],
   "source": [
    "english_example = train_set.filter(lambda example: example['language'] == 'english')\n",
    "print(english_example[:]['question_text'][0]) # The question text\n",
    "print(english_example[:]['document_plaintext'][0]) # The document, which may have the answer to the question\n",
    "print(english_example[:]['annotations'][0]) # The answer to the document - empty if the answer is not within the document text.\n",
    "print('\\n')\n",
    "print(english_example[:]['question_text'][-1])\n",
    "print(english_example[:]['document_plaintext'][-1])\n",
    "print(english_example[:]['annotations'][-1]) # this one will be empty: the answer is not in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrr}\n",
      "\\hline\n",
      " Language   & Dataset        &   Number of questions &   Avg. Words per question &   Answerable ratio &   Avg. no. of words in questions &   No. of unique words \\\\\n",
      "\\hline\n",
      " english    & Train Set      &                  7389 &                      8.06 &                0.5 &                            59533 &                  5018 \\\\\n",
      "            & Validation Set &                   990 &                      8.2  &                0.5 &                             8122 &                  1230 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "table_data = []\n",
    "def unique_words(data):\n",
    "  word_to_ix = {}\n",
    "  for sent in data:\n",
    "      for word in sent:\n",
    "          if word not in word_to_ix:\n",
    "              word_to_ix[word] = len(word_to_ix)\n",
    "  return word_to_ix\n",
    "for language in languages:\n",
    "    train_set = train_set_dict[language]\n",
    "    val_set = val_set_dict[language]\n",
    "\n",
    "    train_questions = train_set['question_words']\n",
    "    val_questions = val_set['question_words']\n",
    "\n",
    "    stop_words = set(stopwords.words(language.lower()))\n",
    "\n",
    "    train_word_counts = len(train_questions)\n",
    "    val_word_counts = len(val_questions)\n",
    "    train_answerable_ratio = np.mean([len(x['annotations']['answer_text'][0]) > 0 for x in train_set])\n",
    "    val_answerable_ratio = np.mean([len(nltk.word_tokenize(x['annotations']['answer_text'][0])) > 0 for x in val_set])\n",
    "\n",
    "    flat_train_q = [item for sublist in train_questions for item in sublist]\n",
    "    flat_val_q = [item for sublist in val_questions for item in sublist]\n",
    "\n",
    "    # Calculate the total length of inner lists\n",
    "    train_avg_words_per_question = sum(len(inner_list) for inner_list in train_questions) / len(train_questions)\n",
    "    val_avg_words_per_question = sum(len(inner_list) for inner_list in val_questions) / len(val_questions)\n",
    "\n",
    "    train_total_words = train_word_counts * train_avg_words_per_question\n",
    "    val_total_words = val_word_counts * val_avg_words_per_question\n",
    "\n",
    "    train_unique_words = len(unique_words(train_questions))\n",
    "    val_unique_words = len(unique_words(val_questions))\n",
    "\n",
    "    table_data.append([\n",
    "        language,\n",
    "        \"Train Set\",\n",
    "        len(train_questions),\n",
    "        np.round(train_avg_words_per_question, 2),\n",
    "        np.round(train_answerable_ratio, 2),\n",
    "        np.round(train_total_words, 2),\n",
    "        np.round(train_unique_words, 2)\n",
    "    ])\n",
    "    table_data.append([\n",
    "        \"\",\n",
    "        \"Validation Set\",\n",
    "        len(val_questions),\n",
    "        np.round(val_avg_words_per_question, 2),\n",
    "        np.round(val_answerable_ratio, 2),\n",
    "        np.round(val_total_words, 2),\n",
    "        np.round(val_unique_words, 2)\n",
    "    ])\n",
    "\n",
    "headers = [\"Language\", \"Dataset\", \"Number of questions\", \"Avg. Words per question\", \"Answerable ratio\", \"Avg. no. of words in questions\", \"No. of unique words\"]\n",
    "\n",
    "table = tabulate(table_data, headers, tablefmt=\"latex_raw\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent words by language in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language      Type Top Word  Count\n",
      "0  English       doc      The   7661\n",
      "1  English       doc       In   2162\n",
      "2  English       doc    first   1875\n",
      "3  English       doc        1   1802\n",
      "4  English       doc        2   1521\n",
      "5  English  question     When   2242\n",
      "6  English  question     What   2103\n",
      "7  English  question      How   1300\n",
      "8  English  question      Who   1070\n",
      "9  English  question    first    948\n"
     ]
    }
   ],
   "source": [
    "combined_data = {'Language': [], 'Type': [], 'Top Word': [], 'Count': []}\n",
    "for country in languages:\n",
    "    stop_words = set(stopwords.words(country.lower()))\n",
    "\n",
    "    # we decided to remove additional characters that were top words\n",
    "    # for the countries because they are not particulary interesting\n",
    "    additional_characters = list(string.punctuation) + [\"``\", \"''\", \"؟\", \",\", \"'s\"]\n",
    "\n",
    "    doc_words = [word for sublist in train_set_dict[country]['doc_text_words'] for word in sublist if word not in stop_words and word not in additional_characters]\n",
    "    question_words = [word for sublist in train_set_dict[country]['question_words'] for word in sublist if word not in stop_words and word not in additional_characters]\n",
    "\n",
    "    top_words_doc = pd.Series(doc_words).value_counts()[:5]\n",
    "    top_words_q = pd.Series(question_words).value_counts()[:5]\n",
    "\n",
    "\n",
    "    # we translate to english to understand which words are used the most\n",
    "    \n",
    "    if language == 'english': # If we are analyzing the english data set, there is no reason to translate\n",
    "        translated_words_doc = top_words_doc.index\n",
    "        translated_words_q = top_words_q.index\n",
    "    else:\n",
    "        translator = Translator()\n",
    "\n",
    "        translated_words_doc = [translator.translate(word, src=country, dest='en').text for word in top_words_doc.index]\n",
    "        translated_words_q = [translator.translate(word, src=country, dest='en').text for word in top_words_q.index]\n",
    "\n",
    "    for word, count in zip(translated_words_doc, top_words_doc.values):\n",
    "        combined_data['Language'].append(country.capitalize())\n",
    "        combined_data['Type'].append('doc')\n",
    "        combined_data['Top Word'].append(word)\n",
    "        combined_data['Count'].append(count)\n",
    "\n",
    "    for word, count in zip(translated_words_q, top_words_q.values):\n",
    "        combined_data['Language'].append(country.capitalize())\n",
    "        combined_data['Type'].append('question')\n",
    "        combined_data['Top Word'].append(word)\n",
    "        combined_data['Count'].append(count)\n",
    "\n",
    "combined_df = pd.DataFrame(combined_data)\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A initial model: A rule-based classifier\n",
    "The model is supervised and predicts if a document contains an answer to a question. If the document and question share 2 or more words, it predicts \"yes\"; otherwise, \"no.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the english language: 0.62\n"
     ]
    }
   ],
   "source": [
    "def oracle(data, language):\n",
    "    \"\"\"Generates labels based on whether answers are present in annotations.\"\"\"\n",
    "    return [0 if elem['annotations']['answer_text'][0] == '' else 1 for elem in data[language]]\n",
    "\n",
    "def has_sufficient_common_words(question_words, doc_words, n):\n",
    "    \"\"\"Checks if a question is answerable by counting common words with the document.\"\"\"\n",
    "    return len(set(question_words) & set(doc_words)) >= n\n",
    "\n",
    "def rule_based_classifier(data, country, n):\n",
    "    \"\"\"\n",
    "    Classifies questions as answerable or not based on common words between question and document.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Dataset containing question and document words.\n",
    "        country (str): Language identifier \n",
    "        n (int): Minimum number of common words required to classify as answerable.\n",
    "    \n",
    "    Returns:\n",
    "        list: Binary classifications for each question.\n",
    "    \"\"\"\n",
    "    country_data = data[country]\n",
    "    question_words = country_data['question_words']\n",
    "    doc_words = country_data['doc_text_words']\n",
    "\n",
    "    return [has_sufficient_common_words(q, doc_words[i], n) for i, q in enumerate(question_words)]\n",
    "\n",
    "def compute_accuracies(data, languages, n):\n",
    "    accuracies = {}\n",
    "    for lang in languages:\n",
    "        predictions = rule_based_classifier(data, lang, n)\n",
    "        labels = oracle(data, lang)\n",
    "        accuracy = np.mean(np.array(predictions) == np.array(labels))\n",
    "        accuracies[lang] = np.round(accuracy, 2)\n",
    "    return accuracies\n",
    "\n",
    "accuracies = compute_accuracies(val_set_dict, languages, 2)\n",
    "\n",
    "# Print accuracies for each language\n",
    "for lang, acc in accuracies.items():\n",
    "    print(f\"Accuracy for the {lang} language: {acc}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
